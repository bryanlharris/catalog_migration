{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad6dc6b5",
   "metadata": {},
   "source": [
    "# Migrate Views Between Catalogs\n",
    "This notebook copies all views from a source catalog to a destination catalog. Provide the catalog names below and run all cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70c048a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%md\n",
    "## Configure Source and Destination Catalogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a294b2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    dbutils.widgets.text(\"1.source_catalog\", \"source_catalog\")\n",
    "    dbutils.widgets.text(\"2.destination_catalog\", \"destination_catalog\")\n",
    "    source_catalog = dbutils.widgets.get(\"1.source_catalog\")\n",
    "    destination_catalog = dbutils.widgets.get(\"2.destination_catalog\")\n",
    "except NameError:\n",
    "    # When running as a script\n",
    "    source_catalog = \"source_catalog\"\n",
    "    destination_catalog = \"destination_catalog\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fe2c69",
   "metadata": {},
   "source": [
    "## Migrate Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b89f705",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()\n",
    "\n",
    "\n",
    "def get_views(catalog, view_type):\n",
    "    query = f\"\"\"\n",
    "    SELECT table_schema, table_name, view_definition\n",
    "    FROM `{catalog}`.information_schema.{view_type}\n",
    "    WHERE table_schema NOT IN ('information_schema')\n",
    "    \"\"\"\n",
    "    return spark.sql(query)\n",
    "\n",
    "\n",
    "views = get_views(source_catalog, \"views\")\n",
    "\n",
    "\n",
    "def recreate_views(df, view_kind):\n",
    "    for row in df.collect():\n",
    "        schema = row[\"table_schema\"]\n",
    "        name = row[\"table_name\"]\n",
    "        definition = row[\"view_definition\"].rstrip(\";\")\n",
    "        spark.sql(f\"CREATE SCHEMA IF NOT EXISTS `{destination_catalog}`.`{schema}`\")\n",
    "        spark.sql(f\"CREATE OR REPLACE {view_kind} `{destination_catalog}`.`{schema}`.`{name}` AS {definition}\")\n",
    "        print(f\"Created {view_kind} {destination_catalog}.{schema}.{name}\")\n",
    "\n",
    "\n",
    "recreate_views(views, \"VIEW\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "environmentMetadata": {
    "environment_version": "3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}