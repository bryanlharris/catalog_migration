{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db8cac2b-cb65-4a66-8269-b98083c68b68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Catalog Table Sizes\n",
    "This notebook lists the total size of every table across all schemas in a selected catalog using [DiscoverX](https://github.com/databrickslabs/discoverx).\n",
    "\n",
    "Use the widget below to select one or more catalogs, then run the remaining cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f693cfd-c252-4d56-8f15-aa37e15da2ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install dbl-discoverx\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7043fef2-38f4-48ee-8c4a-e3f088f5f9ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create widgets for catalogs\n",
    "catalogs = [row.catalog for row in spark.sql(\"SHOW CATALOGS\").collect()]\n",
    "catalogs.append(\"None Selected\")\n",
    "dbutils.widgets.multiselect(\"1.catalogs\", \"None Selected\", catalogs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92f6ad14-cd9f-4a9e-a91c-1988a39a548b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog_list = [c for c in dbutils.widgets.get(\"1.catalogs\").split(',') if c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ffe525f-d583-41a6-9834-cfee32e59ded",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from discoverx import DX\n",
    "\n",
    "dx = DX()\n",
    "\n",
    "def human_size(size_bytes):\n",
    "    for unit in ['B','KB','MB','GB','TB','PB','EB']:\n",
    "        if size_bytes < 1024 or unit == 'EB':\n",
    "            return f\"{size_bytes:.2f} {unit}\"\n",
    "        size_bytes /= 1024\n",
    "\n",
    "def table_size(tbl):\n",
    "    qname = f\"`{tbl.catalog}`.`{tbl.schema}`.`{tbl.table}`\"\n",
    "    df = spark.sql(f\"DESCRIBE DETAIL {qname}\")\n",
    "    size = df.select('sizeInBytes').collect()[0][0]\n",
    "    return {\n",
    "        'table': f\"{tbl.catalog}.{tbl.schema}.{tbl.table}\",\n",
    "        'size': size,\n",
    "        'size_human': human_size(size)\n",
    "    }\n",
    "\n",
    "results = []\n",
    "for cat in catalog_list:\n",
    "    results.extend(dx.from_tables(f'{cat}.*.*').map(table_size))\n",
    "df = spark.createDataFrame(results)\n",
    "\n",
    "# Add total row for all tables\n",
    "total_size = df.agg({'size': 'sum'}).collect()[0][0]\n",
    "df = df.union(spark.createDataFrame([Row(size=total_size, size_human=human_size(total_size), table='ALL_TABLES')]))\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b31ac83c-8e87-46a2-9933-500eb9e18bb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Aggregate table sizes to the schema level\n",
    "schema_sizes = (\n",
    "    df.filter(F.col(\"table\") != \"ALL_TABLES\")\n",
    "      .withColumn(\"catalog\", F.split(F.col(\"table\"), \"\\\\.\").getItem(0))\n",
    "      .withColumn(\"schema\", F.split(F.col(\"table\"), \"\\\\.\").getItem(1))\n",
    "      .groupBy(\"catalog\", \"schema\")\n",
    "      .agg(F.sum(\"size\").alias(\"size\"))\n",
    ")\n",
    "\n",
    "schema_sizes.createOrReplaceTempView(\"schema_sizes\")\n",
    "\n",
    "# Use ai_similarity to build a graph where every edge represents two schemas\n",
    "# whose names are similar enough to belong to the same dataset. The\n",
    "# connected components of this graph capture groups of any size (2 or more)\n",
    "# so a schema can be linked indirectly through intermediate matches.\n",
    "similar_pairs = (\n",
    "    spark.sql(\n",
    "        \"\"\"\n",
    "        SELECT\n",
    "            a.catalog AS catalog_a,\n",
    "            a.schema AS schema_a,\n",
    "            b.catalog AS catalog_b,\n",
    "            b.schema AS schema_b,\n",
    "            ai_similarity(\n",
    "                REGEXP_REPLACE(LOWER(a.schema), 'raw|nmls|dms', ''),\n",
    "                REGEXP_REPLACE(LOWER(b.schema), 'raw|nmls|dms', '')\n",
    "            ) AS similarity\n",
    "        FROM schema_sizes a\n",
    "        CROSS JOIN schema_sizes b\n",
    "        WHERE a.schema <= b.schema\n",
    "        \"\"\"\n",
    "    )\n",
    "    .filter(F.col(\"similarity\") >= 0.85)\n",
    "    .select(\"catalog_a\", \"schema_a\", \"catalog_b\", \"schema_b\")\n",
    ")\n",
    "\n",
    "pairs = [\n",
    "    (f\"{row.catalog_a}.{row.schema_a}\", f\"{row.catalog_b}.{row.schema_b}\")\n",
    "    for row in similar_pairs.collect()\n",
    "]\n",
    "\n",
    "schema_rows = schema_sizes.select(\"catalog\", \"schema\").collect()\n",
    "nodes = [f\"{row.catalog}.{row.schema}\" for row in schema_rows]\n",
    "\n",
    "parents = {node: node for node in nodes}\n",
    "\n",
    "def find(node):\n",
    "    while parents[node] != node:\n",
    "        parents[node] = parents[parents[node]]\n",
    "        node = parents[node]\n",
    "    return node\n",
    "\n",
    "\n",
    "def union(a, b):\n",
    "    root_a, root_b = find(a), find(b)\n",
    "    if root_a == root_b:\n",
    "        return\n",
    "    if root_a < root_b:\n",
    "        parents[root_b] = root_a\n",
    "    else:\n",
    "        parents[root_a] = root_b\n",
    "\n",
    "\n",
    "for a, b in pairs:\n",
    "    union(a, b)\n",
    "\n",
    "clusters = {}\n",
    "for node in nodes:\n",
    "    root = find(node)\n",
    "    clusters.setdefault(root, []).append(node)\n",
    "\n",
    "dataset_rows = []\n",
    "for members in clusters.values():\n",
    "    members.sort()\n",
    "    schema_names = [m.split(\".\", 1)[1] for m in members]\n",
    "    label = \", \".join(schema_names)\n",
    "    for member in members:\n",
    "        catalog, schema = member.split(\".\", 1)\n",
    "        dataset_rows.append((catalog, schema, label))\n",
    "\n",
    "dataset_assignments = spark.createDataFrame(dataset_rows, [\"catalog\", \"schema\", \"dataset\"])\n",
    "\n",
    "human_size_udf = F.udf(human_size)\n",
    "\n",
    "dataset_totals = (\n",
    "    schema_sizes.join(dataset_assignments, [\"catalog\", \"schema\"], \"inner\")\n",
    "                .groupBy(\"dataset\")\n",
    "                .agg(F.sum(\"size\").alias(\"size\"))\n",
    "                .withColumn(\"size_human\", human_size_udf(\"size\"))\n",
    "                .orderBy(F.desc(\"size\"))\n",
    ")\n",
    "\n",
    "display(dataset_totals.select(\"dataset\", \"size\", \"size_human\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "165c16ab-fee3-47f0-a372-6090a28bf67b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# (\n",
    "#     df.write\n",
    "#     .mode(\"overwrite\")\n",
    "#     .option(\"overwriteSchema\", \"true\")\n",
    "#     .saveAsTable(\"staging.storage_costs.prod_tables\")\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "catalog_table_sizes",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
