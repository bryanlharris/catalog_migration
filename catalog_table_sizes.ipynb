{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f78daae-48a6-4008-942b-797b08b48d5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Catalog Table Sizes\n",
    "This notebook lists the total size of every table across all schemas in a selected catalog using [DiscoverX](https://github.com/databrickslabs/discoverx).\n",
    "\n",
    "Use the widget below to select one or more catalogs, then run the remaining cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "61676e63-a607-432d-a8ac-6cbe4a7d9532",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install dbl-discoverx\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "146c5177-b2ed-444d-afce-a448d6b35ddf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create widgets for catalogs\n",
    "catalogs = [row.catalog for row in spark.sql(\"SHOW CATALOGS\").collect()]\n",
    "catalogs.append(\"None Selected\")\n",
    "dbutils.widgets.multiselect(\"1.catalogs\", \"None Selected\", catalogs)\n",
    "\n",
    "dbutils.widgets.text(\"2.target_table\", \"\", label=\"Target table (catalog.schema.table)\")\n",
    "dbutils.widgets.dropdown(\"2.write_mode\", \"overwrite\", [\"overwrite\", \"append\"], label=\"Write mode\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64f932d5-9be8-421d-adb0-eda09c6eddb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog_list = [c for c in dbutils.widgets.get(\"1.catalogs\").split(',') if c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a57d842-03a0-47f8-b8ec-c8598979ce90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from discoverx import DX\n",
    "\n",
    "dx = DX()\n",
    "\n",
    "def human_size(size_bytes):\n",
    "    for unit in ['B','KB','MB','GB','TB','PB','EB']:\n",
    "        if size_bytes < 1024 or unit == 'EB':\n",
    "            return f\"{size_bytes:.2f} {unit}\"\n",
    "        size_bytes /= 1024\n",
    "\n",
    "def table_size(tbl):\n",
    "    qname = f\"`{tbl.catalog}`.`{tbl.schema}`.`{tbl.table}`\"\n",
    "    df = spark.sql(f\"DESCRIBE DETAIL {qname}\")\n",
    "    size = df.select('sizeInBytes').collect()[0][0]\n",
    "    return {\n",
    "        'table': f\"{tbl.catalog}.{tbl.schema}.{tbl.table}\",\n",
    "        'size': size,\n",
    "        'size_human': human_size(size)\n",
    "    }\n",
    "\n",
    "results = []\n",
    "for cat in catalog_list:\n",
    "    results.extend(dx.from_tables(f'{cat}.*.*').map(table_size))\n",
    "df = spark.createDataFrame(results)\n",
    "\n",
    "# Add total row for all tables\n",
    "total_size = df.agg({'size': 'sum'}).collect()[0][0]\n",
    "df = df.union(spark.createDataFrame([Row(size=total_size, size_human=human_size(total_size), table='ALL_TABLES')]))\n",
    "target_table = dbutils.widgets.get(\"2.target_table\").strip()\n",
    "write_mode = dbutils.widgets.get(\"2.write_mode\")\n",
    "\n",
    "if target_table:\n",
    "    (df\n",
    "     .write\n",
    "     .mode(write_mode)\n",
    "     .option(\"overwriteSchema\", \"true\")\n",
    "     .saveAsTable(target_table)\n",
    "     )\n",
    "\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "catalog_table_sizes",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
