{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29ce980a-b6b6-49ed-b16e-ae3da165b8fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Backup a Catalog\n",
    "This notebook uses [DiscoverX](https://github.com/databrickslabs/discoverx) to clone all schemas and tables from a source catalog into a destination catalog using Delta Lake `CLONE`.\n",
    "After cloning, it removes any tables or schemas in the destination that no longer exist in the source.\n",
    "\n",
    "Specify the source and destination catalogs with the widgets below and run all cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "887cbe65-0102-494e-bd56-789f3b5d3d72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install dbl-discoverx\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6bf3bef-d65c-4b1a-b6d5-623f1120b78c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configure source and destination catalogs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41139b85-2447-4fe9-9e1a-1715e890cf0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create widgets for user input using drop-downs populated from existing catalogs\n",
    "catalogs = ['None Selected'] + [row.catalog for row in spark.sql(\"SHOW CATALOGS\").collect()]\n",
    "dbutils.widgets.combobox(\"1.source_catalog\", \"None Selected\", catalogs, label=\"1. Source Catalog\")\n",
    "dbutils.widgets.combobox(\"2.destination_catalog\", \"None Selected\", catalogs, label=\"2. Destination Catalog\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41139b85-2447-4fe9-9e1a-1715e890cf0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "source_catalog = dbutils.widgets.get(\"1.source_catalog\")\n",
    "destination_catalog = dbutils.widgets.get(\"2.destination_catalog\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41139b85-2447-4fe9-9e1a-1715e890cf0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "allowed_catalogs = {'dev', 'staging', 'prod'}\n",
    "if source_catalog not in allowed_catalogs:\n",
    "    raise ValueError(f'Source catalog must be one of {sorted(allowed_catalogs)}')\n",
    "expected_dest = f'{source_catalog}_backup'\n",
    "if destination_catalog != expected_dest:\n",
    "    raise ValueError(f'Destination catalog must be {expected_dest}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "011325da-7764-4c50-a07a-ec7deeb90fd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Clone all tables using DiscoverX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4dc4db8-da03-4bdb-9495-6e4cd34ef3bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from discoverx import DX\n",
    "\n",
    "dx = DX()\n",
    "\n",
    "def clone_table(table_info):\n",
    "    \"\"\"Clone a single table into the destination catalog.\"\"\"\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS `{destination_catalog}`.`{table_info.schema}`\")\n",
    "    try:\n",
    "        spark.sql(\n",
    "            f\"\"\"CREATE OR REPLACE TABLE `{destination_catalog}`.`{table_info.schema}`.`{table_info.table}` CLONE `{table_info.catalog}`.`{table_info.schema}`.`{table_info.table}`\"\"\"\n",
    "        )\n",
    "        return {\n",
    "            \"source\": f\"`{table_info.catalog}`.`{table_info.schema}`.`{table_info.table}`\",\n",
    "            \"destination\": f\"`{destination_catalog}`.`{table_info.schema}`.`{table_info.table}`\",\n",
    "            \"success\": True,\n",
    "            \"info\": None,\n",
    "        }\n",
    "    except Exception as err:\n",
    "        return {\n",
    "            \"source\": f\"`{table_info.catalog}`.`{table_info.schema}`.`{table_info.table}`\",\n",
    "            \"destination\": f\"`{destination_catalog}`.`{table_info.schema}`.`{table_info.table}`\",\n",
    "            \"success\": False,\n",
    "            \"info\": str(err),\n",
    "        }\n",
    "\n",
    "\n",
    "# Apply clone function to all tables in the source catalog\n",
    "results = dx.from_tables(f\"{source_catalog}.*.*\").map(clone_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0f5eb9d-d4e7-4d93-9ace-9358814493ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Show cloning results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff300547-3602-4ba2-83d1-30f691cdf3e9",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"source\":383,\"destination\":364},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1753814985173}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField,StringType,BooleanType\n",
    "schema=StructType([\n",
    "    StructField(\"source\",StringType(),True),\n",
    "    StructField(\"destination\",StringType(),True),\n",
    "    StructField(\"success\",BooleanType(),False),\n",
    "    StructField(\"info\",StringType(),True)\n",
    "])\n",
    "df = spark.createDataFrame(results,schema)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "auto-generated-perms-md",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Migrate Schema and Table Permissions\n",
    "Copy permissions from the source catalog to the destination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": false,
     "inputWidgets": {},
     "nuid": "auto-generated-perms-code",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema_query = f\"\"\"\n",
    "SELECT schema_name, grantee, privilege_type\n",
    "FROM system.information_schema.schema_privileges\n",
    "WHERE catalog_name = '{source_catalog}'\n",
    "\"\"\"\n",
    "\n",
    "table_query = f\"\"\"\n",
    "SELECT table_schema, table_name, grantee, privilege_type\n",
    "FROM system.information_schema.table_privileges\n",
    "WHERE table_catalog = '{source_catalog}'\n",
    "\"\"\"\n",
    "\n",
    "schema_df = spark.sql(schema_query)\n",
    "table_df = spark.sql(table_query)\n",
    "\n",
    "grant_cmds = []\n",
    "for row in schema_df.collect():\n",
    "    if row.schema_name.lower() == 'information_schema':\n",
    "        continue\n",
    "    object_identifier = f\"`{destination_catalog}`.`{row.schema_name}`\"\n",
    "    grant_cmds.append(f\"GRANT {row.privilege_type} ON SCHEMA {object_identifier} TO `{row.grantee}`;\")\n",
    "\n",
    "for row in table_df.collect():\n",
    "    if row.table_schema.lower() == 'information_schema':\n",
    "        continue\n",
    "    object_identifier = f\"`{destination_catalog}`.`{row.table_schema}`.`{row.table_name}`\"\n",
    "    grant_cmds.append(f\"GRANT {row.privilege_type} ON TABLE {object_identifier} TO `{row.grantee}`;\")\n",
    "\n",
    "for cmd in grant_cmds:\n",
    "    print(cmd)\n",
    "    spark.sql(cmd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "131fa9cf-efcd-49bb-a7df-e59750085b76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Remove stale tables from the destination catalog\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4f91955-838f-4d88-bc9e-67612f5b07d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Determine tables present only in the destination catalog\n",
    "source_df = spark.sql(f\"\"\"\n",
    "    SELECT table_schema, table_name\n",
    "    FROM `{source_catalog}`.information_schema.tables\n",
    "    WHERE table_schema NOT IN ('information_schema')\n",
    "\"\"\"\n",
    ")\n",
    "dest_df = spark.sql(f\"\"\"\n",
    "    SELECT table_schema, table_name\n",
    "    FROM `{destination_catalog}`.information_schema.tables\n",
    "    WHERE table_schema NOT IN ('information_schema')\n",
    "\"\"\"\n",
    ")\n",
    "source_tables = {(r.table_schema, r.table_name) for r in source_df.collect()}\n",
    "dest_tables = {(r.table_schema, r.table_name) for r in dest_df.collect()}\n",
    "\n",
    "obsolete_tables = dest_tables - source_tables\n",
    "for schema, table in sorted(obsolete_tables):\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS `{destination_catalog}`.`{schema}`.`{table}`\")\n",
    "    print(f'Dropped `{destination_catalog}`.`{schema}`.`{table}`')\n",
    "\n",
    "# Drop any empty schemas that remain\n",
    "schema_df = spark.sql(f'SHOW SCHEMAS IN `{destination_catalog}`')\n",
    "for row in schema_df.collect():\n",
    "    schema = row.schemaName if hasattr(row, 'schemaName') else row[0]\n",
    "    if schema == 'information_schema':\n",
    "        continue\n",
    "    if not spark.sql(f'SHOW TABLES IN `{destination_catalog}`.`{schema}`').collect():\n",
    "        spark.sql(f'DROP SCHEMA IF EXISTS `{destination_catalog}`.`{schema}`')\n",
    "        print(f'Dropped schema `{destination_catalog}`.`{schema}`')\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "backup_catalog",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}