{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a49ea17",
   "metadata": {},
   "source": [
    "# Change Catalog Object Owners\n",
    "This notebook uses [DiscoverX](https://github.com/databrickslabs/discoverx) to change the owner of every object in a catalog.\n",
    "\n",
    "Provide the catalog and the new owner below, then run all cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1e854d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install dbl-discoverx\n",
    "dbutils.library.restartpython\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d837d44",
   "metadata": {},
   "source": [
    "## Configure Catalog and New Owner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe6d42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Widgets for user input\n",
    "try:\n",
    "    dbutils.widgets.text(\"1.catalog\", \"source_catalog\")\n",
    "    dbutils.widgets.text(\"2.new_owner\", \"account users\")\n",
    "    catalog = dbutils.widgets.get(\"1.catalog\")\n",
    "    new_owner = dbutils.widgets.get(\"2.new_owner\")\n",
    "except NameError:\n",
    "    # When running as a Python script\n",
    "    catalog = \"source_catalog\"\n",
    "    new_owner = \"account users\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf4645d",
   "metadata": {},
   "source": [
    "## Change Schema Owners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e451daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "schemas_df = spark.sql(f\"SHOW SCHEMAS IN `{catalog}`\")\n",
    "for row in schemas_df.collect():\n",
    "    schema = row[0]\n",
    "    spark.sql(f\"ALTER SCHEMA `{catalog}`.`{schema}` OWNER TO `{new_owner}`\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58eb0ce5",
   "metadata": {},
   "source": [
    "## Change Table Owners using DiscoverX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443bf2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from discoverx import DX\n",
    "\n",
    "dx = DX()\n",
    "\n",
    "def set_table_owner(tbl_info):\n",
    "    try:\n",
    "        spark.sql(\n",
    "            f\"ALTER TABLE `{tbl_info.catalog}`.`{tbl_info.schema}`.`{tbl_info.table}` OWNER TO `{new_owner}`\"\n",
    "        )\n",
    "        return {\n",
    "            \"table\": f\"`{tbl_info.catalog}`.`{tbl_info.schema}`.`{tbl_info.table}`\",\n",
    "            \"success\": True,\n",
    "            \"info\": None,\n",
    "        }\n",
    "    except Exception as err:\n",
    "        return {\n",
    "            \"table\": f\"`{tbl_info.catalog}`.`{tbl_info.schema}`.`{tbl_info.table}`\",\n",
    "            \"success\": False,\n",
    "            \"info\": str(err),\n",
    "        }\n",
    "\n",
    "results = dx.from_tables(f\"{catalog}.*.*\").map(set_table_owner)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfe330a",
   "metadata": {},
   "source": [
    "## Change Volume Owners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e50749",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "volume_df = spark.sql(f\"\"\"\n",
    "SELECT volume_schema, volume_name\n",
    "FROM system.information_schema.volumes\n",
    "WHERE volume_catalog = '{catalog}'\n",
    "\"\"\")\n",
    "for row in volume_df.collect():\n",
    "    spark.sql(\n",
    "        f\"ALTER VOLUME `{catalog}`.`{row.volume_schema}`.`{row.volume_name}` OWNER TO `{new_owner}`\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79466134",
   "metadata": {},
   "source": [
    "## Change Function Owners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971882f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "func_df = spark.sql(f\"\"\"\n",
    "SELECT routine_schema, routine_name\n",
    "FROM system.information_schema.routines\n",
    "WHERE routine_catalog = '{catalog}'\n",
    "\"\"\")\n",
    "for row in func_df.collect():\n",
    "    spark.sql(\n",
    "        f\"ALTER FUNCTION `{catalog}`.`{row.routine_schema}`.`{row.routine_name}` OWNER TO `{new_owner}`\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e758485c",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac3d35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.display()"
   ]
  }
 ],
 "metadata": "metadata": {
   "application/vnd.databricks.v1+notebook": {
     "computePreferences": null,
     "dashboards": [],
     "environmentMetadata": {
       "environment_version": "3"
     },
     "inputWidgetPreferences": null,
     "language": "sql",
     "notebookMetadata": {
       "pythonIndentUnit": 4
     },
     "notebookName": "Volumes",
     "widgets": {}
   },
   "language_info": {
     "name": "sql"
   }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
